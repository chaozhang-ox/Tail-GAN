"""
Compute the rejection rate based on coverage test and score test for the synthetic data generated by TailGAN.
"""

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
plt.style.use("ggplot")
import numpy as np
import pandas as pd
import torch
import os
from os.path import *
from matplotlib.backends.backend_pdf import PdfPages
import seaborn as sns
import openpyxl
from openpyxl import load_workbook
import random
from scipy.stats import chi2
from scipy.stats import ttest_ind

from TailGAN import *
from Dataset import Dataset_IS
from Transform import *
from gen_thresholds import gen_thresholds

your_path = 'your_path'  # Replace with your actual path
sample_number = 1000


def Empirical_Stats(data):
    """
    Compute the empirical VaR and ES for either true data or synthetic data
    """
    ep_stats_l = []
    for alpha in opt.alphas:
        var_l = np.percentile(data, alpha * 100, axis=(0))
        var = np.round(var_l, 3)
        var_l = var_l.reshape(1, *var_l.shape)
        if alpha < 0.5:
            tmp_data = data * (data <= var_l)
            tmp_data[tmp_data == 0.0] = np.nan
            es = np.round(np.nanmean(tmp_data, axis=(0)), 3)
        else:
            tmp_data = data * (data >= var_l)
            tmp_data[tmp_data == 0.0] = np.nan
            es = np.round(np.nanmean(tmp_data, axis=(0)), 3)
        ep_stats_l.extend(np.stack([var, es]))

    rt = np.array(ep_stats_l).T
    rt = np.round(rt, 3)
    return rt


def Compute_PNL_NP_IS(R):
    R_tensor = Tensor(R)
    # Price
    prices_l = Inc2Price(R_tensor)
    port_prices_l = StaticPort(prices_l, 50, opt.static_way, insample=True)

    PNL_BH = BuyHold(prices_l, opt.Cap)
    PNL_l = [PNL_BH]
    columns = ['Stk-%d ' % (i + 1) for i in range(PNL_BH.shape[1])]

    thresholds_pct = [[31, 69]] # opt.thresholds_pct

    strategies = ['Port', 'MR', 'TF'] # opt.strategies

    for strategy in strategies:
        if strategy == 'Port':
            PNL_BHPort = BuyHold(port_prices_l, opt.Cap)
            PNL_l.append(PNL_BHPort)
            columns.extend(['Trans-%d ' % (i + 1) for i in range(PNL_BHPort.shape[1])])
        elif strategy == 'MR':
            for percentile_l in thresholds_pct:
                thresholds_array = gen_thresholds(opt.data_name, opt.tickers, strategy, percentile_l, 100, opt.WH)
                PNL_MR = MeanRev(prices_l, opt.Cap, opt.WH, LR=opt.ratios[0], SR=opt.ratios[1],
                                 ST=thresholds_array[:, -1], LT=thresholds_array[:, -2])
                PNL_l.append(PNL_MR)
                columns.extend(['MR-%d ' % (i + 1) for i in range(PNL_MR.shape[1])])
        elif strategy == 'TF':
            for percentile_l in thresholds_pct:
                thresholds_array = gen_thresholds(opt.data_name, opt.tickers, strategy, percentile_l, 100, opt.WH)
                PNL_TF = TrendFollow(prices_l, opt.Cap, opt.WH, LR=opt.ratios[0], SR=opt.ratios[1],
                                     ST=thresholds_array[:, 0], LT=thresholds_array[:, 1])
                PNL_l.append(PNL_TF)
                columns.extend(['TF-%d ' % (i + 1) for i in range(PNL_TF.shape[1])])
        else:
            pass

    PNL = torch.cat(PNL_l, dim=1)
    return PNL.cpu().detach().numpy(), columns


def Load_Data(opt, model_index):
    # Synthetic Data
    dataset = Dataset_IS(tickers=opt.tickers, data_path=join(your_path, "gan_data", opt.data_name), length=opt.len)
    hist_dataset = Dataset_IS(tickers=opt.tickers, data_path=join(your_path, "gan_data", opt.data_name), length=int(opt.len * 0.1))

    real = np.array([d.detach().numpy() for d in dataset.samples])
    hist_real = np.array([d.detach().numpy() for d in hist_dataset.samples])

    # Fake Data
    epoches_l = []
    fake_l = []
    files = os.listdir(gen_data_path)
    epoches = [int(s.split('_E')[1][:-4]) for s in files if s.startswith('Fake') and s.endswith('.npy') and 'id%d' % model_index in s]
    epoches.sort()

    for i in epoches[::10]:
        tmp_fake_l = []
        idx = epoches.index(i)
        for j in epoches[idx:(idx+10)]:
            tmp_fake0 = np.load(join(gen_data_path, 'Fake_id%d_E%d.npy' % (model_index, j)))
            tmp_fake_l.append(tmp_fake0)

        tmp_fake = np.concatenate(tmp_fake_l)
        fake_l.append(tmp_fake)
        epoches_l.append(i)

    return real, hist_real, fake_l, epoches_l


############ Compute or Load the GroundTruth Estimates from 100k examples ###################
def VaR_ES_Real_Stats(opt, real):
    index_l = ['Real']
    is_stats_l = []
    sub_path = '_'.join(['Synthetic',
                         '_'.join(opt.strategies),
                         'P' + str(opt.n_trans),
                         'Cap' + str(opt.Cap),
                         'WH' + str(opt.WH),
                         'R' + '+'.join([str(a) for a in opt.ratios]),
                         'T' + '+'.join(['_'.join(map(str, i)) for i in opt.thresholds_pct]),
                         'estimates_OOS.csv'])
    real_estimates_path = join(your_path, 'GroundTruth', sub_path)
    sub_var_path = '_'.join(['Synthetic',
                         '_'.join(opt.strategies),
                         'P' + str(opt.n_trans),
                         'Cap' + str(opt.Cap),
                         'WH' + str(opt.WH),
                         'R' + '+'.join([str(a) for a in opt.ratios]),
                         'T' + '+'.join(['_'.join(map(str, i)) for i in opt.thresholds_pct]),
                         'OOS_N%d.csv' % sample_number])
    real_path = join(your_path, 'GroundTruth', sub_var_path)

    # # # # # # # # Synthetic Estimates # # # # # # # # #
    if isfile(real_path):
        print('GroundTruth Estimates Exist!')
        is_real_ve = pd.read_csv(real_estimates_path, index_col=0)
        is_stats_l.append(np.concatenate([is_real_ve.values]))
        pnl_df = pd.read_csv(real_path, index_col=0)
    else:
        print('Creating GroundTruth Estimates ......')
        # In-Sample
        PNL, is_clms_l = Compute_PNL_NP_IS(real)

        # Storage
        pnl_df = pd.DataFrame(np.round(PNL, 3), columns=is_clms_l)
        pnl_df.to_csv(real_path)

        is_real_ve = Empirical_Stats(PNL).reshape(-1)
        ep_clms_l = []
        for alpha in opt.alphas:
            ep_clms_l.extend(['VaR_%.2f' % alpha, 'ES_%.2f' % alpha])

        is_final_clms = [i + j for i in is_clms_l for j in ep_clms_l]
        is_stats_l.append(np.concatenate([is_real_ve]))

        # Storage
        df = pd.DataFrame(np.round(np.column_stack(is_stats_l).T, 3), index=index_l, columns=is_final_clms)
        df.to_csv(real_estimates_path)

    return is_stats_l, index_l, pnl_df


def Coverage_test(pnl_df, var_estimates):
    breach_df = pnl_df < var_estimates
    s_n = breach_df.sum(0)
    n = breach_df.shape[0]
    alpha = opt.alphas[0]
    lr_uc = -2 * ((n - s_n) * np.log(1-alpha) + s_n * np.log(alpha) - (n - s_n) * np.log(1 - s_n / n) - s_n * np.log(s_n / n))
    p_value = lr_uc.apply(lambda x: 1-chi2.cdf(x, 1))
    rej_rate = p_value < 0.05
    return rej_rate


def VaR_Rej(opt, real, hist_real, fake_l, epoches_l):
    is_stats_l, index_l, pnl_df = VaR_ES_Real_Stats(opt, real)

    # # # # # # # # Sample Estimates # # # # # # # # #
    is_sample_ve_l = []

    for i in range(100):
        if sample_number > real.shape[0]:
            sample_real = real
        else:
            sample_idx = random.sample(range(real.shape[0]), sample_number)
            sample_real = real[sample_idx, :, :]
        sample_PNL, _ = Compute_PNL_NP_IS(sample_real)
        real_sample_ve = Empirical_Stats(sample_PNL).reshape(-1)
        real_sample_ve = np.concatenate([real_sample_ve])
        var_estimates = real_sample_ve[::2]

        if sample_number > pnl_df.shape[0]:
            sub_pnl_df = pnl_df
        else:
            sample_idx = random.sample(range(pnl_df.shape[0]), sample_number)
            sub_pnl_df = pnl_df.iloc[sample_idx, :]

        rej_rate = Coverage_test(sub_pnl_df, var_estimates)
        is_sample_ve_l.append(rej_rate)

    err_l = []
    err_index_l = []

    err_l.append(np.mean(np.stack(is_sample_ve_l, axis=1), axis=1))
    err_index_l.append('FS-Rej-Mean')

    is_sample_ve_l = []

    for i in range(100):
        if sample_number > hist_real.shape[0]:
            sample_real = hist_real
        else:
            sample_idx = random.sample(range(hist_real.shape[0]), sample_number)
            sample_real = hist_real[sample_idx, :, :]
        sample_PNL, _ = Compute_PNL_NP_IS(sample_real)
        real_sample_ve = Empirical_Stats(sample_PNL).reshape(-1)
        real_sample_ve = np.concatenate([real_sample_ve])
        var_estimates = real_sample_ve[::2]
        if sample_number > pnl_df.shape[0]:
            sub_pnl_df = pnl_df
        else:
            sample_idx = random.sample(range(pnl_df.shape[0]), sample_number)
            sub_pnl_df = pnl_df.iloc[sample_idx, :]

        rej_rate = Coverage_test(sub_pnl_df, var_estimates)
        is_sample_ve_l.append(rej_rate)

    # # # # # # # # Compute Relative Error # # # # # # # # # #
    # Sample Relative Error
    err_l.append(np.mean(np.stack(is_sample_ve_l, axis=1), axis=1))
    err_index_l.append('HS-Rej-Mean')

    # Generated Relative Error
    for i in range(len(fake_l)-20, len(fake_l)):
        fake_PNL, is_clms_l = Compute_PNL_NP_IS(fake_l[i])
        is_sample_ve_l = []

        for j in range(100):
            if sample_number > fake_PNL.shape[0]:
                sub_fake_PNL = fake_PNL
            else:
                sample_idx = random.sample(range(fake_PNL.shape[0]), sample_number)
                sub_fake_PNL = fake_PNL[sample_idx, :]

            fake_ve = Empirical_Stats(sub_fake_PNL).reshape(-1)
            var_estimates = fake_ve[::2]

            if sample_number > pnl_df.shape[0]:
                sub_pnl_df = pnl_df
            else:
                sample_idx = random.sample(range(pnl_df.shape[0]), sample_number)
                sub_pnl_df = pnl_df.iloc[sample_idx, :]

            rej_rate = Coverage_test(sub_pnl_df, var_estimates)
            is_sample_ve_l.append(rej_rate)

        err_l.append(np.mean(np.stack(is_sample_ve_l, axis=1), axis=1))
        err_index_l.append('E%d' % epoches_l[i] + '-Rej')

    ep_clms_l = []
    for alpha in opt.alphas:
        ep_clms_l.extend(['VaR_%.2f' % alpha, 'ES_%.2f' % alpha])

    is_final_clms = [i+j for i in is_clms_l for j in ep_clms_l]

    df = pd.DataFrame(np.round(np.column_stack(err_l).T, 3), index=err_index_l, columns=is_final_clms[::2])
    os.makedirs("/data01/Chao_TailGAN/Results/%s/" % this_version, exist_ok=True)
    save_path = "/data01/Chao_TailGAN/Results/%s/Tails_IS_Rej_N%d.csv" % (this_version, sample_number)
    df.to_csv(save_path)
    print(' * '*10+' Coverage test '+' * '*10)
    print(df)
    return df


def G1_quant(v, W=opt.W):
    return - W * v ** 2 / 2


def G2_quant(e, alpha):
    return alpha * e


def G2in_quant(e, alpha):
    return alpha * e ** 2 / 2


def S_quant(v, e, X, alpha, W=opt.W):
    """
    For a given quantile, here named alpha, calculate the score function value
    """
    if alpha < 0.5:
        rt = ((X<=v) - alpha) * (G1_quant(v,W) - G1_quant(X,W)) + 1. / alpha * G2_quant(e,alpha) * (X<=v) * (v - X) + G2_quant(e,alpha) * (e - v) - G2in_quant(e,alpha)
    else:
        alpha_inverse = 1 - alpha
        rt = ((X>=v) - alpha_inverse) * (G1_quant(v,W) - G1_quant(X,W)) + 1. / alpha_inverse * G2_quant(-e,alpha_inverse) * (X>=v) * (X - v) + G2_quant(-e,alpha_inverse) * (v - e) - G2in_quant(-e,alpha_inverse)
    return rt


def Score_test(pnl_df, ve_estimates, real_sample_ve):
    v = ve_estimates[::2]
    e = ve_estimates[1::2]
    hs_v = real_sample_ve[::2]
    hs_e = real_sample_ve[1::2]
    alpha = opt.alphas[0]

    rej_rate_l = []
    for i in range(len(v)):
        s_model = S_quant(v[i:i+1], e[i:i+1], pnl_df.values[:, i:i+1], alpha)
        s_hs = S_quant(hs_v[i:i + 1], hs_e[i:i + 1], pnl_df.values[:, i:i + 1], alpha)
        t_value, p_value = ttest_ind(s_hs, s_model, equal_var=False)

        rej_rate_l.append((p_value < 0.05))
    rej_rate_df = pd.DataFrame(rej_rate_l).T
    rej_rate_df.columns = pnl_df.columns

    return rej_rate_df


def VaR_ES_Rej(opt, real, hist_real, fake_l, epoches_l):
    is_stats_l, index_l, pnl_df = VaR_ES_Real_Stats(opt, real)

    # # # # # # # # Compute Relative Error # # # # # # # # # #
    # Sample Relative Error
    err_l = []
    err_index_l = []

    is_sample_ve_l = []

    for i in range(100):
        if sample_number > hist_real.shape[0]:
            sample_real = hist_real
        else:
            sample_idx = random.sample(range(hist_real.shape[0]), sample_number)
            sample_real = hist_real[sample_idx, :, :]
            
        sample_PNL, _ = Compute_PNL_NP_IS(sample_real)
        histreal_sample_ve = Empirical_Stats(sample_PNL).reshape(-1)
        histreal_sample_ve = np.concatenate([histreal_sample_ve])

        if sample_number > real.shape[0]:
            sample_real = real
        else:
            sample_idx = random.sample(range(real.shape[0]), sample_number)
            sample_real = real[sample_idx, :, :]

        sample_PNL, _ = Compute_PNL_NP_IS(sample_real)
        real_sample_ve = Empirical_Stats(sample_PNL).reshape(-1)
        real_sample_ve = np.concatenate([real_sample_ve])

        if sample_number > pnl_df.shape[0]:
            sub_pnl_df = pnl_df
        else:
            sample_idx = random.sample(range(pnl_df.shape[0]), sample_number)
            sub_pnl_df = pnl_df.iloc[sample_idx, :]

        rej_rate = Score_test(sub_pnl_df, histreal_sample_ve, real_sample_ve)
        is_sample_ve_l.append(rej_rate)

    # # # # # # # # Compute Relative Error # # # # # # # # # #
    # Sample Relative Error
    err_l.append(np.mean(np.stack(is_sample_ve_l, axis=1), axis=1))
    err_index_l.append('HS-Rej-Mean')
    
    # Generated Relative Error
    for i in range(len(fake_l)-20, len(fake_l)):
        fake_PNL, is_clms_l = Compute_PNL_NP_IS(fake_l[i])

        tmp_rej_rate_l = []
        for j in range(100):
            if sample_number > fake_PNL.shape[0]:
                sub_fake_PNL = fake_PNL
            else:
                sample_idx = random.sample(range(fake_PNL.shape[0]), nnn*sample_number)
                sub_fake_PNL = fake_PNL[sample_idx, :]

            ve_estimates = Empirical_Stats(sub_fake_PNL).reshape(-1)
            ve_estimates = np.concatenate([ve_estimates])

            if sample_number > real.shape[0]:
                sample_real = real
            else:
                sample_idx = random.sample(range(real.shape[0]), sample_number)
                sample_real = real[sample_idx, :, :]

            sample_PNL, _ = Compute_PNL_NP_IS(sample_real)
            real_sample_ve = Empirical_Stats(sample_PNL).reshape(-1)
            real_sample_ve = np.concatenate([real_sample_ve])

            if sample_number > pnl_df.shape[0]:
                sub_pnl_df = pnl_df
            else:
                sample_idx = random.sample(range(pnl_df.shape[0]), sample_number)
                sub_pnl_df = pnl_df.iloc[sample_idx, :]

            rej_rate = Score_test(sub_pnl_df, ve_estimates, real_sample_ve)
            tmp_rej_rate_l.append(rej_rate.astype(int))

        err_l.append(np.row_stack(tmp_rej_rate_l).mean(0))
        err_index_l.append('E%d' % epoches_l[i] + '-Rej')

    ep_clms_l = []
    for alpha in opt.alphas:
        ep_clms_l.extend(['VaR_%.2f' % alpha, 'ES_%.2f' % alpha])

    is_final_clms = [i+j for i in is_clms_l for j in ep_clms_l]

    df = pd.DataFrame(np.round(np.row_stack(err_l), 3), index=err_index_l, columns=is_final_clms[::2])
    os.makedirs(join(your_path, "Results/%s/" % this_version), exist_ok=True)
    save_path = join(your_path, "Results/%s/Tails_IS_Rej_N%d_Score.csv" % (this_version, sample_number))
    df.to_csv(save_path)
    print(' * '*10+' Score test '+' * '*10)
    print(df)
    return df


if __name__ == "__main__":
    select_l = Screen_Ensemble()
    real, hist_real, fake_l, epoches_l = Load_Data(opt, select_l[0])
    coverage_df = VaR_Rej(opt, real, hist_real, fake_l, epoches_l)
    score_df = VaR_ES_Rej(opt, real, hist_real, fake_l, epoches_l)
